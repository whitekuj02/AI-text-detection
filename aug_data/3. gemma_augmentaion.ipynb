{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, LongformerForSequenceClassification, LongformerTokenizer, DebertaV2ForSequenceClassification\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import Trainer, TrainingArguments, PreTrainedTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "import re\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from libauc.losses import AUCMLoss\n",
    "from libauc.optimizers import PESG\n",
    "import transformers\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data_base\": \"../data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(f\"{CONFIG['data_base']}/pseudo_labeling.csv\")\n",
    "test_csv = pd.read_csv(f\"{CONFIG['data_base']}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = train_csv.rename(columns={\n",
    "    'paragraph': 'full_text',\n",
    "    'paragraph_label': 'generated'\n",
    "    })\n",
    "\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = train_csv[train_csv['generated'] == 0] # 0 or 1\n",
    "label_0 = label_0.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gen(text):\n",
    "    return (\n",
    "        f\"이 문장을 비슷한 문장으로 다시 작성해줘.\\n\"\n",
    "        + f\"다른 부수적인 말하지 말고 오직 비슷한 문장 1개만 말해주면 돼.\\n\"\n",
    "        + f\"문장: {text}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, pipeline):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_gen(text)}\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():  # memory leak 방지\n",
    "        outputs = pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=2048,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    torch.cuda.empty_cache()  # GPU 캐시 비움\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "augmentation = {\n",
    "    \"title\" : [],\n",
    "    \"full_text\" : [],\n",
    "    \"generated\" : [],\n",
    "}\n",
    "\n",
    "for _, i in tqdm(label_0.iterrows(), total=len(label_0)):\n",
    "    title = i['title']\n",
    "    text = i['full_text']\n",
    "    label = i['generated']\n",
    "\n",
    "    aug_text = generate(text, pipeline)\n",
    "\n",
    "    augmentation['title'].append(title)\n",
    "    augmentation['full_text'].append(aug_text.strip().replace(\"\\n\", \"\"))\n",
    "    augmentation['generated'].append(1)\n",
    "\n",
    "    pd_aug = pd.DataFrame(augmentation)\n",
    "    pd_aug.to_csv(\"./augmentation_data/aug_0.csv\", index=False)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "construct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
