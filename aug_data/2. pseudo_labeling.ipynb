{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 데이터를 Paragraph 단위로 자른 후 Knowledge base 가 큰 LLM 으로 relabeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{data_base}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"../labeling_ckpt/checkpoint-200000\"  # 원하는 checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "\n",
    "\n",
    "def response_postprocessing(decoded):\n",
    "    # 공백 제거\n",
    "    decoded = decoded.strip()\n",
    "\n",
    "    # 뒤쪽 100자만 잘라서 보면 속도도 빠르고 의미도 보장됨\n",
    "    tail = decoded[-100:].upper()  # 대소문자 구분 없게\n",
    "\n",
    "    if re.search(r'\\bAI\\b', tail[::-1]):\n",
    "        return 1\n",
    "    elif re.search(r'\\bHUMAN\\b', tail[::-1]):\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(sentence, tokenizer, model):\n",
    "    valid_keys = {\"input_ids\", \"attention_mask\"}\n",
    "\n",
    "    inputs = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            stride=256,  # ✅ 겹치게 자름\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # segment 중 하나 랜덤 선택\n",
    "    n_segments = inputs[\"input_ids\"].size(0)\n",
    "    seg_idx = random.randint(0, n_segments - 1)\n",
    "\n",
    "    item = {\n",
    "        k: v[seg_idx] for k, v in inputs.items() if k != \"overflow_to_sample_mapping\"\n",
    "    }\n",
    "\n",
    "    inputs = {k: v.unsqueeze(0).to(model.device) for k, v in item.items() if k in valid_keys}\n",
    "\n",
    "    # Longformer는 token_type_ids 없음\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = f\"{data_base}/pseudo_labeling.csv\"\n",
    "# 초기 딕셔너리\n",
    "parsing_df = {\n",
    "    \"title\": [],\n",
    "    \"paragraph_idx\": [],\n",
    "    \"paragraph\": [],\n",
    "    \"paragraph_label\": [],\n",
    "    \"document_label\": []\n",
    "}\n",
    "\n",
    "for _, row in tqdm(train_df.iterrows(), desc=\"processing\", total=len(train_df)):\n",
    "    title = row[\"title\"]\n",
    "    text = row[\"full_text\"]\n",
    "    document_label = row[\"generated\"]\n",
    "\n",
    "    split_text = split_sentences(text)\n",
    "\n",
    "    for idx, paragraph in enumerate(split_text):\n",
    "        if document_label == 0:\n",
    "            paragraph_label = 0\n",
    "        else:\n",
    "            paragraph_label = make(paragraph, tokenizer, model).cpu().item()\n",
    "\n",
    "        # 딕셔너리에 추가\n",
    "        parsing_df[\"title\"].append(title)\n",
    "        parsing_df[\"paragraph_idx\"].append(idx)\n",
    "        parsing_df[\"paragraph\"].append(paragraph)\n",
    "        parsing_df[\"paragraph_label\"].append(paragraph_label)\n",
    "        parsing_df[\"document_label\"].append(document_label)\n",
    "\n",
    "# 최종 DataFrame 생성\n",
    "parsing_df = pd.DataFrame(parsing_df)\n",
    "parsing_df.to_csv(f\"{output_csv}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(output_csv)\n",
    "train_csv = train_csv.rename(columns={\n",
    "    'paragraph': 'full_text'\n",
    "    })\n",
    "\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"데이터 개수 : {len(train_csv)}\")\n",
    "avg_text = sum([len(i) for i in train_csv[\"full_text\"]])/len(train_csv)\n",
    "print(f\"평균 text 길이 : {avg_text}\")\n",
    "max_length = max([len(i) for i in train_csv[\"full_text\"]])\n",
    "min_length = min([len(i) for i in train_csv[\"full_text\"]])\n",
    "print(f\"가장 길이가 긴거 : {max_length}\")\n",
    "print(f\"가장 길이가 짧은거 : {min_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"클래스 비율 : {sum(train_csv['paragraph_label'])/len(train_csv)}\")\n",
    "print(f\"가중치 : {len(train_csv)/sum(train_csv['paragraph_label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = train_csv[train_csv['document_label'] == train_csv['paragraph_label']]\n",
    "\n",
    "list_1_1 = []\n",
    "for _,i in filter_df.iterrows():\n",
    "    if i['paragraph_label'] == 1:\n",
    "        list_1_1.append(i)\n",
    "\n",
    "print(f\"1 == 1 : {len(list_1_1)}\")\n",
    "print(f\"0 == 1 : {len(train_csv[train_csv['document_label'] != train_csv['paragraph_label']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 먼저 generated == 1 인 row만 필터링\n",
    "gen1_df = train_csv[train_csv[\"document_label\"] == 1]\n",
    "\n",
    "# 2. 그 중 title 별로 paragraph_label 평균 계산\n",
    "grouped = gen1_df.groupby(\"title\")[\"paragraph_label\"].mean()\n",
    "\n",
    "# 3. 평균이 정확히 0인 title만 선택\n",
    "zero_titles = grouped[grouped == 0].index.tolist()\n",
    "\n",
    "print(f\"paragraph_label 평균이 0인 title 개수: {len(zero_titles)}\")\n",
    "print(\"예시:\", zero_titles[:10])\n",
    "\n",
    "# 전체 title 수 (중복 제거)\n",
    "total_titles = train_csv[\"title\"].nunique()\n",
    "\n",
    "# generated == 1 인 title만 필터링\n",
    "generated_1_titles = train_csv[train_csv[\"document_label\"] == 1][\"title\"].unique()\n",
    "\n",
    "# 개수와 비율 계산\n",
    "num_generated_1 = len(zero_titles)\n",
    "ratio = num_generated_1 / total_titles\n",
    "\n",
    "print(f\"전체 title 수: {total_titles}\")\n",
    "print(f\"generated == 1 인 title 수: {num_generated_1}\")\n",
    "print(f\"비율: {ratio:.4f} ({ratio*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = train_csv[~train_csv[\"title\"].isin(zero_titles)]\n",
    "filtered_df.to_csv(output_csv, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "construct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
