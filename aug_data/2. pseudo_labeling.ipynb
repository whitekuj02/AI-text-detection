{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 데이터를 Paragraph 단위로 자른 후 Knowledge base 가 큰 LLM 으로 relabeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>카호올라웨섬</td>\n",
       "      <td>카호올라웨섬은 하와이 제도를 구성하는 8개의 화산섬 가운데 하나로 면적은 115.5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>청색거성</td>\n",
       "      <td>천문학에서 청색거성(靑色巨星, )은 광도 분류에서 III형(거성) 또는 II형(밝은...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>엘자스-로트링겐 평의회 공화국</td>\n",
       "      <td>엘자스-로트링겐 평의회 공화국은 1차대전 말기 독일 혁명 와중에 엘자스-로트링겐에서...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>윌리엄 페니 브룩스</td>\n",
       "      <td>윌리엄 페니 브룩스(, 1809년 8월 13일 ~ 1895년 12월 11일)는 잉글...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미그로</td>\n",
       "      <td>미그로 또는 미그로스(\"Migros\")는 스위스 최대 소매 회사이자, 최대 슈퍼마켓...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                                          full_text  \\\n",
       "0            카호올라웨섬  카호올라웨섬은 하와이 제도를 구성하는 8개의 화산섬 가운데 하나로 면적은 115.5...   \n",
       "1              청색거성  천문학에서 청색거성(靑色巨星, )은 광도 분류에서 III형(거성) 또는 II형(밝은...   \n",
       "2  엘자스-로트링겐 평의회 공화국  엘자스-로트링겐 평의회 공화국은 1차대전 말기 독일 혁명 와중에 엘자스-로트링겐에서...   \n",
       "3        윌리엄 페니 브룩스  윌리엄 페니 브룩스(, 1809년 8월 13일 ~ 1895년 12월 11일)는 잉글...   \n",
       "4               미그로  미그로 또는 미그로스(\"Migros\")는 스위스 최대 소매 회사이자, 최대 슈퍼마켓...   \n",
       "\n",
       "   generated  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{data_base}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 08:47:33.164393: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-18 08:47:33.172099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752796053.180439 3261954 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752796053.182738 3261954 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-18 08:47:33.191727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ModulesToSaveWrapper(\n",
       "    (original_module): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    )\n",
       "    (modules_to_save): ModuleDict(\n",
       "      (default): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = f\"../labeling_ckpt/checkpoint-200000\"  # 원하는 checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "\n",
    "\n",
    "def response_postprocessing(decoded):\n",
    "    # 공백 제거\n",
    "    decoded = decoded.strip()\n",
    "\n",
    "    # 뒤쪽 100자만 잘라서 보면 속도도 빠르고 의미도 보장됨\n",
    "    tail = decoded[-100:].upper()  # 대소문자 구분 없게\n",
    "\n",
    "    if re.search(r'\\bAI\\b', tail[::-1]):\n",
    "        return 1\n",
    "    elif re.search(r'\\bHUMAN\\b', tail[::-1]):\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(sentence, tokenizer, model):\n",
    "    valid_keys = {\"input_ids\", \"attention_mask\"}\n",
    "\n",
    "    inputs = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            stride=256,  # ✅ 겹치게 자름\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # segment 중 하나 랜덤 선택\n",
    "    n_segments = inputs[\"input_ids\"].size(0)\n",
    "    seg_idx = random.randint(0, n_segments - 1)\n",
    "\n",
    "    item = {\n",
    "        k: v[seg_idx] for k, v in inputs.items() if k != \"overflow_to_sample_mapping\"\n",
    "    }\n",
    "\n",
    "    inputs = {k: v.unsqueeze(0).to(model.device) for k, v in item.items() if k in valid_keys}\n",
    "\n",
    "    # Longformer는 token_type_ids 없음\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:   0%|          | 0/97172 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 97172/97172 [44:50<00:00, 36.12it/s]  \n"
     ]
    }
   ],
   "source": [
    "output_csv = f\"{data_base}/pseudo_labeling.csv\"\n",
    "# 초기 딕셔너리\n",
    "parsing_df = {\n",
    "    \"title\": [],\n",
    "    \"paragraph_idx\": [],\n",
    "    \"paragraph\": [],\n",
    "    \"paragraph_label\": [],\n",
    "    \"document_label\": []\n",
    "}\n",
    "\n",
    "for _, row in tqdm(train_df.iterrows(), desc=\"processing\", total=len(train_df)):\n",
    "    title = row[\"title\"]\n",
    "    text = row[\"full_text\"]\n",
    "    document_label = row[\"generated\"]\n",
    "\n",
    "    split_text = split_sentences(text)\n",
    "\n",
    "    for idx, paragraph in enumerate(split_text):\n",
    "        if document_label == 0:\n",
    "            paragraph_label = 0\n",
    "        else:\n",
    "            paragraph_label = make(paragraph, tokenizer, model).cpu().item()\n",
    "\n",
    "        # 딕셔너리에 추가\n",
    "        parsing_df[\"title\"].append(title)\n",
    "        parsing_df[\"paragraph_idx\"].append(idx)\n",
    "        parsing_df[\"paragraph\"].append(paragraph)\n",
    "        parsing_df[\"paragraph_label\"].append(paragraph_label)\n",
    "        parsing_df[\"document_label\"].append(document_label)\n",
    "\n",
    "# 최종 DataFrame 생성\n",
    "parsing_df = pd.DataFrame(parsing_df)\n",
    "parsing_df.to_csv(f\"{output_csv}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph_idx</th>\n",
       "      <th>full_text</th>\n",
       "      <th>paragraph_label</th>\n",
       "      <th>document_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>카호올라웨섬</td>\n",
       "      <td>0</td>\n",
       "      <td>카호올라웨섬은 하와이 제도를 구성하는 8개의 화산섬 가운데 하나로 면적은 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>카호올라웨섬</td>\n",
       "      <td>1</td>\n",
       "      <td>마우이섬에서 남서쪽으로 약 11km 정도 떨어진 곳에 위치하며 라나이섬의 남동쪽에 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>카호올라웨섬</td>\n",
       "      <td>2</td>\n",
       "      <td>1000년경부터 사람이 거주했으며 해안 지대에는 소규모 임시 어촌이 형성되었다. 섬...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>카호올라웨섬</td>\n",
       "      <td>3</td>\n",
       "      <td>1830년대에는 하와이 왕국의 카메하메하 3세 국왕에 의해 남자 죄수들의 유형지로 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>카호올라웨섬</td>\n",
       "      <td>4</td>\n",
       "      <td>1910년부터 1918년까지 하와이 준주가 섬의 원래 모습을 복원하기 위해 이 섬을...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   title  paragraph_idx  \\\n",
       "0           0  카호올라웨섬              0   \n",
       "1           1  카호올라웨섬              1   \n",
       "2           2  카호올라웨섬              2   \n",
       "3           3  카호올라웨섬              3   \n",
       "4           4  카호올라웨섬              4   \n",
       "\n",
       "                                           full_text  paragraph_label  \\\n",
       "0  카호올라웨섬은 하와이 제도를 구성하는 8개의 화산섬 가운데 하나로 면적은 115.5...                0   \n",
       "1  마우이섬에서 남서쪽으로 약 11km 정도 떨어진 곳에 위치하며 라나이섬의 남동쪽에 ...                0   \n",
       "2  1000년경부터 사람이 거주했으며 해안 지대에는 소규모 임시 어촌이 형성되었다. 섬...                0   \n",
       "3  1830년대에는 하와이 왕국의 카메하메하 3세 국왕에 의해 남자 죄수들의 유형지로 ...                0   \n",
       "4  1910년부터 1918년까지 하와이 준주가 섬의 원래 모습을 복원하기 위해 이 섬을...                0   \n",
       "\n",
       "   document_label  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(output_csv)\n",
    "train_csv = train_csv.rename(columns={\n",
    "    'paragraph': 'full_text'\n",
    "    })\n",
    "\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수 : 1226364\n",
      "평균 text 길이 : 181.50835804051653\n",
      "가장 길이가 긴거 : 19124\n",
      "가장 길이가 짧은거 : 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"데이터 개수 : {len(train_csv)}\")\n",
    "avg_text = sum([len(i) for i in train_csv[\"full_text\"]])/len(train_csv)\n",
    "print(f\"평균 text 길이 : {avg_text}\")\n",
    "max_length = max([len(i) for i in train_csv[\"full_text\"]])\n",
    "min_length = min([len(i) for i in train_csv[\"full_text\"]])\n",
    "print(f\"가장 길이가 긴거 : {max_length}\")\n",
    "print(f\"가장 길이가 짧은거 : {min_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 비율 : 0.03529294728155751\n",
      "가중치 : 28.33427290790629\n"
     ]
    }
   ],
   "source": [
    "print(f\"클래스 비율 : {sum(train_csv['paragraph_label'])/len(train_csv)}\")\n",
    "print(f\"가중치 : {len(train_csv)/sum(train_csv['paragraph_label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 == 1 : 43282\n",
      "0 == 1 : 57430\n"
     ]
    }
   ],
   "source": [
    "filter_df = train_csv[train_csv['document_label'] == train_csv['paragraph_label']]\n",
    "\n",
    "list_1_1 = []\n",
    "for _,i in filter_df.iterrows():\n",
    "    if i['paragraph_label'] == 1:\n",
    "        list_1_1.append(i)\n",
    "\n",
    "print(f\"1 == 1 : {len(list_1_1)}\")\n",
    "print(f\"0 == 1 : {len(train_csv[train_csv['document_label'] != train_csv['paragraph_label']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph_label 평균이 0인 title 개수: 185\n",
      "예시: ['1967–1970', '2013년 AFC 챔피언스리그', '2017년 일본 시리즈', '2021년 아프리카 네이션스컵', 'Around the Fur', 'B사감과 러브레터', 'C3 식물', 'E.T.', 'IC 1101', 'IPTG']\n",
      "전체 title 수: 97172\n",
      "generated == 1 인 title 수: 185\n",
      "비율: 0.0019 (0.19%)\n"
     ]
    }
   ],
   "source": [
    "# 1. 먼저 generated == 1 인 row만 필터링\n",
    "gen1_df = train_csv[train_csv[\"document_label\"] == 1]\n",
    "\n",
    "# 2. 그 중 title 별로 paragraph_label 평균 계산\n",
    "grouped = gen1_df.groupby(\"title\")[\"paragraph_label\"].mean()\n",
    "\n",
    "# 3. 평균이 정확히 0인 title만 선택\n",
    "zero_titles = grouped[grouped == 0].index.tolist()\n",
    "\n",
    "print(f\"paragraph_label 평균이 0인 title 개수: {len(zero_titles)}\")\n",
    "print(\"예시:\", zero_titles[:10])\n",
    "\n",
    "# 전체 title 수 (중복 제거)\n",
    "total_titles = train_csv[\"title\"].nunique()\n",
    "\n",
    "# generated == 1 인 title만 필터링\n",
    "generated_1_titles = train_csv[train_csv[\"document_label\"] == 1][\"title\"].unique()\n",
    "\n",
    "# 개수와 비율 계산\n",
    "num_generated_1 = len(zero_titles)\n",
    "ratio = num_generated_1 / total_titles\n",
    "\n",
    "print(f\"전체 title 수: {total_titles}\")\n",
    "print(f\"generated == 1 인 title 수: {num_generated_1}\")\n",
    "print(f\"비율: {ratio:.4f} ({ratio*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = train_csv[~train_csv[\"title\"].isin(zero_titles)]\n",
    "filtered_df.to_csv(output_csv, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "construct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
