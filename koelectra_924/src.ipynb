{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document 단위의 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 20:43:28.510140: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-19 20:43:28.517846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752925408.527233 3321215 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752925408.530034 3321215 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-19 20:43:28.539984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, LongformerForSequenceClassification, LongformerTokenizer, DebertaV2ForSequenceClassification\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import Trainer, TrainingArguments, PreTrainedTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "import re\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data_base\": \"../data\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(f\"{CONFIG['data_base']}/final_aug_train.csv\")\n",
    "test_csv = pd.read_csv(f\"{CONFIG['data_base']}/test.csv\")\n",
    "test_csv = test_csv.rename(columns={\n",
    "    'paragraph_text': 'full_text'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 샘플링 완료: 총 382508개 (각 클래스 54644개)\n",
      "generated\n",
      "0    327864\n",
      "1     54644\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph_idx</th>\n",
       "      <th>full_text</th>\n",
       "      <th>generated</th>\n",
       "      <th>document_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1146518.0</td>\n",
       "      <td>1146518.0</td>\n",
       "      <td>1147756.0</td>\n",
       "      <td>오다 노부유키</td>\n",
       "      <td>6.0</td>\n",
       "      <td>고지(弘治) 2년(1556년) 4월, 노부나가의 지원자였던 장인 사이토 도산(斎藤道...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>625140.0</td>\n",
       "      <td>625140.0</td>\n",
       "      <td>625879.0</td>\n",
       "      <td>신당역 살인 사건</td>\n",
       "      <td>12.0</td>\n",
       "      <td>서울교통공사는 2022년 9월 15일 오전 7시경 직원들에게 특이사항이 없다고 보고...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>714042.0</td>\n",
       "      <td>714042.0</td>\n",
       "      <td>714894.0</td>\n",
       "      <td>먼로주의</td>\n",
       "      <td>2.0</td>\n",
       "      <td>영국은 신성동맹의 정책에 반대한다는 뜻을 분명히 했습니다. 중남미 무역이 엄청난 이...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194033.0</td>\n",
       "      <td>194033.0</td>\n",
       "      <td>194224.0</td>\n",
       "      <td>닭볶음탕</td>\n",
       "      <td>10.0</td>\n",
       "      <td>또, 이 요리는 간혹 조리 전에 간단히 볶아 기름을 녹이는 과정을 제외하면 볶음이라...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>355044.0</td>\n",
       "      <td>355044.0</td>\n",
       "      <td>355457.0</td>\n",
       "      <td>카르멘 (소설)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>주인공은 고고학자로서, 문다 전투의 유적을 답사하기 위해 안내인 안토니오와 같이 스...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0      title  paragraph_idx  \\\n",
       "0     1146518.0     1146518.0   1147756.0    오다 노부유키            6.0   \n",
       "1      625140.0      625140.0    625879.0  신당역 살인 사건           12.0   \n",
       "2      714042.0      714042.0    714894.0       먼로주의            2.0   \n",
       "3      194033.0      194033.0    194224.0       닭볶음탕           10.0   \n",
       "4      355044.0      355044.0    355457.0   카르멘 (소설)            3.0   \n",
       "\n",
       "                                           full_text  generated  \\\n",
       "0  고지(弘治) 2년(1556년) 4월, 노부나가의 지원자였던 장인 사이토 도산(斎藤道...          0   \n",
       "1  서울교통공사는 2022년 9월 15일 오전 7시경 직원들에게 특이사항이 없다고 보고...          0   \n",
       "2  영국은 신성동맹의 정책에 반대한다는 뜻을 분명히 했습니다. 중남미 무역이 엄청난 이...          1   \n",
       "3  또, 이 요리는 간혹 조리 전에 간단히 볶아 기름을 녹이는 과정을 제외하면 볶음이라...          0   \n",
       "4  주인공은 고고학자로서, 문다 전투의 유적을 답사하기 위해 안내인 안토니오와 같이 스...          0   \n",
       "\n",
       "   document_label  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             1.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 데이터에서 클래스별 분리\n",
    "label_0 = train_csv[train_csv['generated'] == 0]\n",
    "label_1 = train_csv[train_csv['generated'] == 1]\n",
    "\n",
    "# 두 클래스 중 작은 수로 균형 맞추기\n",
    "count = min(len(label_0), len(label_1))\n",
    "\n",
    "# 무작위 샘플링\n",
    "sampled_0 = label_0.sample(n=6*count, random_state=42)\n",
    "sampled_1 = label_1.sample(n=count, random_state=42)\n",
    "\n",
    "# 균형 잡힌 데이터셋 생성\n",
    "train_csv = pd.concat([sampled_0, sampled_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 확인\n",
    "print(f\"✅ 샘플링 완료: 총 {len(train_csv)}개 (각 클래스 {count}개)\")\n",
    "print(train_csv[\"generated\"].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "0    327864\n",
      "1     54644\n",
      "Name: count, dtype: int64\n",
      "\n",
      "라벨 0 비율: 0.8571\n",
      "라벨 1 비율: 0.1429\n"
     ]
    }
   ],
   "source": [
    "# 각 라벨 개수 출력\n",
    "print(train_csv['generated'].value_counts())\n",
    "\n",
    "# 전체 개수\n",
    "total = len(train_csv)\n",
    "\n",
    "# 각 비율 계산\n",
    "label_0_ratio = (train_csv['generated'] == 0).sum() / total\n",
    "label_1_ratio = (train_csv['generated'] == 1).sum() / total\n",
    "\n",
    "print(f\"\\n라벨 0 비율: {label_0_ratio:.4f}\")\n",
    "print(f\"라벨 1 비율: {label_1_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_csv,\n",
    "    test_size=0.01,\n",
    "    random_state=42,\n",
    "    stratify=train_csv['generated']  # label 분포 유지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, mode='train'):\n",
    "        self.data = data_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['full_text']\n",
    "\n",
    "        # 슬라이딩 윈도우 기반 tokenization\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            stride=256,  # ✅ 겹치게 자름\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # segment 중 하나 랜덤 선택\n",
    "        n_segments = inputs[\"input_ids\"].size(0)\n",
    "        seg_idx = random.randint(0, n_segments - 1)\n",
    "\n",
    "        item = {\n",
    "            k: v[seg_idx] for k, v in inputs.items() if k != \"overflow_to_sample_mapping\"\n",
    "        }\n",
    "\n",
    "        # Longformer는 token_type_ids 없음\n",
    "        item.pop(\"token_type_ids\", None)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            item[\"labels\"] = int(row[\"generated\"])  # binary classification\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "train_dataset = CustomDataset(train_df, tokenizer, mode='train')\n",
    "val_dataset = CustomDataset(val_df, tokenizer, mode='train')  # 선택적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ElectraForSequenceClassification(\n",
       "      (electra): ElectraModel(\n",
       "        (embeddings): ElectraEmbeddings(\n",
       "          (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): ElectraEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x ElectraLayer(\n",
       "              (attention): ElectraAttention(\n",
       "                (self): ElectraSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): ElectraSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): ElectraIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ElectraOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): ElectraClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): ElectraClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"monologg/koelectra-base-v3-discriminator\",\n",
    "    num_labels=2  # AI vs HUMAN → 이진 분류\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],  # BERT 계열은 attention 부분 지정\n",
    "    # target_modules=[\"query_proj\", \"value_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auroc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")  # shape: (B,)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # shape: (B, C)\n",
    "\n",
    "        # Label Smoothing\n",
    "        smoothing = 0.0\n",
    "        num_classes = logits.size(1)\n",
    "        smoothed_labels = F.one_hot(labels, num_classes=num_classes).float()  # (B, C)\n",
    "        smoothed_labels = smoothed_labels * (1 - smoothing) + smoothing / num_classes\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        ce_loss = - (smoothed_labels * torch.log(probs + 1e-8)).sum(dim=-1)  # shape: (B,)\n",
    "\n",
    "        # # Focal Loss (gamma=2)\n",
    "        # gamma = 2.0\n",
    "        # pt = probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)  # p_t\n",
    "        # # pt = (probs * smoothed_labels).sum(dim=-1)  # soft p_t\n",
    "        # focal_weight = (1 - pt) ** gamma  # shape: (B,)\n",
    "\n",
    "        # 클래스별 weight 적용 (0: 1.0, 1: 12.0)\n",
    "        class_weights = torch.tensor([1.0, 6.0], device=logits.device)\n",
    "        example_weights = class_weights[labels]  # shape: (B,)\n",
    "\n",
    "        loss = (ce_loss * example_weights).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3321215/2177980789.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='300000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     9/300000 00:01 < 13:31:39, 6.16 it/s, Epoch 0.00/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WeightedTrainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/construct/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/construct/lib/python3.11/site-packages/transformers/trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    #num_train_epochs=1,\n",
    "    max_steps=300000,\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10000,\n",
    "    logging_dir=\"./\",\n",
    "    logging_steps=10000,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset, \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'qalora_group_size', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_qalora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Running inference without DataLoader:   5%|▍         | 94/1962 [00:00<00:18, 102.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m         preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     29\u001b[0m         all_probs\u001b[38;5;241m.\u001b[39mextend(probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     31\u001b[0m sample_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sample_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataset = CustomDataset(test_csv, tokenizer, mode='eval')\n",
    "\n",
    "# 모델 준비\n",
    "checkpoint_path =\"./checkpoint-220000\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# 예측 저장 리스트\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "valid_keys = {\"input_ids\", \"attention_mask\"}\n",
    "\n",
    "# ✅ 4. 추론 루프 (DataLoader 없이)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset)), desc=\"Running inference without DataLoader\"):\n",
    "        batch = test_dataset[i]  # 단일 샘플 꺼내기\n",
    "        # 모델이 받는 키만 선택\n",
    "        inputs = {k: v.unsqueeze(0).to(model.device) for k, v in batch.items() if k in valid_keys}\n",
    "        inputs.pop(\"token_type_ids\", None)  # 혹시 있을 경우 제거\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_probs.extend(probs.cpu().tolist())\n",
    "\n",
    "sample_submission = pd.read_csv(f\"{CONFIG['data_base']}/sample_submission.csv\", encoding='utf-8-sig')\n",
    "all_AI = [i[1] for i in all_probs]\n",
    "sample_submission['generated'] = all_AI\n",
    "\n",
    "sample_submission.to_csv(f\"submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "construct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
